{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-10-12T14:43:46.383331Z","iopub.execute_input":"2022-10-12T14:43:46.383821Z","iopub.status.idle":"2022-10-12T14:43:46.420523Z","shell.execute_reply.started":"2022-10-12T14:43:46.383732Z","shell.execute_reply":"2022-10-12T14:43:46.419286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This notebook is still working in progress","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nI have done an analysis on the dataset of 2021 survey. Here is the [report](https://medium.com/@tianmin/are-you-an-explorer-a-climber-or-an-expert-in-the-data-science-world-e6c574937f30), if you want to read. This analysis identifies three clusters of 2021 Kaggle survey participants by **k-means clustering** method. By digging further of each cluster, we name them as explorers, climbers and experts, depending on how they respond questions in regards of demographics, professions, their skill and knowledge in data science, the tools they are frequently using and tools they plan to get more familiar in the next two years. In this notebook, I want to reproduce the methodology for the **2022** dataset and try to see if it fits.\n\n## Motivation\n\nThis analysis tries to answer -\n\n1. How many types of professionists in the data science field?\n2. How does each segment of data science professionsts differ in demographics, professions, their skill and knowledge in data science, the tools they are frequently using and tools they plan to get more familiar in the next two years?","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport pyarrow\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# functions\ndef rename_columns(df):\n    \"\"\"\n    input: the dataset we want to rename the columns\n    output: combine the first row of the dataset into the original column\n    \"\"\"\n    original_columns = df.columns\n    num_col = df.shape[1]\n    first_row = df.iloc[0] #grab the first row for the header\n    df = df[1:] #take the data below the first row\n    # create a list containing new column names\n    new_cols = []\n    for col in range(num_col):\n        new_col_name = original_columns[col] + '_' + first_row[col]\n        new_cols.append(new_col_name)\n    df.columns = new_cols # assign the new column names to the dataset\n    return df\n\ndef replace_nan(df):\n    \"\"\"\n    input:\n    df - the target dataset\n    \n    output:\n    a new dataset with nan values replaced as 0 and non-nan values replaced with 1\n    \"\"\"\n    array = np.where(df.isnull(),0,1)\n    df = pd.DataFrame(data=array, columns=df.columns)\n    df.index = df.index + 1\n    return df\n\ndef split_cols(df):\n    \"\"\"\n    input:\n    df - target dataframe\n    \n    output:\n    single_questions - a list of column names that belong to single question column\n    multiple_questions - a list of column names that belong to multiple question column\n    \"\"\"\n    single_questions = []\n    multiple_questions = []\n    for col in df.columns:\n        if 'part' in col.lower() or 'other' in col.lower():\n            multiple_questions.append(col)\n        else:\n            single_questions.append(col)\n    return single_questions, multiple_questions\n\ndef pivot_col(df, col):\n    \"\"\"\n    input:\n    df - target dataset\n    col - the column we want to pivot its value as new columns\n    \n    output:\n    return a pivoted dataframe where columns are value from the col of old dataframe\n    \"\"\"\n    df['participant_id'] = df.index\n    pivoted_df = df.pivot(index = 'participant_id', columns=col, values=col).reset_index().iloc[: , 1:]\n    pivoted_df.index = pivoted_df.index + 1\n    return pivoted_df\n\ndef pivot_df(df):\n    \"\"\"\n    input:\n    df - targer dataframe\n    var_cols - a list of column names we want to pivot\n    aggr - the column we used to group by the dataset\n        \n    output:\n    return a dataframe where each column comes from value of each col of old dataframe\n    NaN value replaced with 0 while non-NaN value replaced with 1\n    \"\"\"\n    \n    pivoted_df = []\n    for col in df.columns:\n        if col in single_questions: ## single question answers\n            pivoted = pivot_col(df[[col]], col)\n            pivoted_df.append(pivoted)\n        else:\n            pivoted_df.append(df[[col]])\n    pivoted_merged_df = pd.concat(pivoted_df, axis=1, ignore_index=False)\n    return pivoted_merged_df\n\ndef closest_participant(participant_id, participant_matrix):\n    \"\"\"\n    input:\n    participant_id - target participant\n    participant_matrix - matrix where shows the similarity between each participant\n    \n    output - the list of participants other than the target participant, ranked by similarity\n    \"\"\"\n    participant_list = participant_matrix[[participant_id]]\n    participant_list = participant_list.sort_values(by = participant_id, ascending = False)\n    \n    return participant_list.index[1:]\n\ndef compensation(df , participant):\n    \"\"\"\n    input -\n    df - target dataset\n    participant - the id of the participant\n    \n    output -\n    the yearly compensation of that participant\n    \"\"\"\n    \n    compensation = df.loc[df.index == participant]['Q25_What is your current yearly compensation (approximate $USD)?'].iloc[0]\n    \n    return compensation\n\ndef similar_user_compensation(df, participant_ids):\n    \"\"\"\n    input:\n    df - target dataset\n    participant_ids - a list of participant ids\n    output:\n    the first participant id that has non-null compensation data\n    \"\"\"\n    for participant in participant_ids:\n        if compensation(df, participant) is not None:\n            return compensation(df, participant)\n        else:\n            pass\n\ndef same_answers(df, user_1, user_2):\n    \"\"\"\n    input\n    df - target dataset\n    user_1 - index number of user 1\n    user_2 - index number of user 2\n    \n    output\n    same_cols - return the column names where answer are same between user 1 and 2\n    different_cols - return the column names where answer are different between user 1 and 2\n    \"\"\"\n    same_cols = []\n    different_cols = []\n    answers = df.loc[df.index.isin([user_1,user_2])]\n    for col in answers.columns:\n        if answers[col].iloc[0] == answers[col].iloc[1]:\n            same_cols.append(col)\n        else:\n            different_cols.append(col)\n    return same_cols, different_cols\n\ndef compute_correlation(df, user1, user2):\n    '''\n    INPUT\n    user1 - int user_id\n    user2 - int user_id\n    df - dataset where is a matrix of user and their pivoted answer columns\n    OUTPUT\n    the correlation between the matching ratings between the two users\n    '''\n    answer_1 = list(df.loc[df.index == user1].iloc[0])\n    answer_2 = list(df.loc[df.index == user2].iloc[0])\n    \n    dot_product = np.vdot(answer_1, answer_2)\n    \n    return dot_product #return the correlation\n\ndef subset_data(df, col, criteria):\n    \"\"\"\n    input:\n    df: the dataset we want to subset from\n    col: target columns as the filter\n    criteria: value to feed the filter\n    \n    output:\n    a new dataset which is a subset of the original one\n    \"\"\"\n    \n    new_df = df.loc[df[col] == criteria]\n    \n    return new_df\n\ndef question_columns(df, query, method = 'strict'):\n    \"\"\"\n    input: \n    df - target dataset\n    query - str, query we want to find relevant infomation in the dataset. e.g. 'Q7', or 'machine learning' \n    \n    output:\n    a subset of data which include the columns of the query in interest\n    \n    method:\n    if it == strict, which means we will look for the question exactly EQUALS to the query. e.g. if we search 'age', then 'language' won't\n    be taken into account in this case;\n    \n    if it == loose, which means we will look for the question exactly CONTAINS the query. e.g. if we search 'age', then 'language' will\n    be taken into account in this case.\n    \"\"\"\n    columns = df.columns\n    question_col = []\n    for col in columns:\n        if method == 'strict':\n            col_parts = col.lower().split() # each column name will be separated into single word tokens at first\n            if query.lower() in col_parts:\n                question_col.append(col)\n        elif method == 'loose':\n            if query.lower() in col.lower():\n                question_col.append(col)\n    return df[question_col]\n\ndef kmeans_cluster_opt(df, init = 'k-means++', max_num_cluster = 9):\n    \"\"\"\n    input: \n    df - the dataset we want to segments into cluster\n    init - the way we want to initialize the starting centroid\n    max_num_cluster - the max number of cluster\n    \n    output:\n    a visualization showing the line graph indicating the optimal number of klusters, based on inertias value\n    \"\"\"\n    num_clusters = list(range(1, max_num_cluster))\n    inertias = []\n\n    for k in num_clusters:\n        model = KMeans(init=init, n_clusters=k, random_state = 42)\n        model.fit(df)\n        inertias.append(model.inertia_)\n\n    \n    plt.plot(num_clusters, inertias, '-o')\n\n    plt.xlabel('number of clusters (k)')\n    plt.ylabel('inertia')\n\n    plt.show()\n\ndef kmeans_predict(df, init = 'k-means++', n_clusters = 4):\n    \"\"\"\n    input:\n    df - dataset we want to segment into clusters\n    init - the way we want to initialize the starting centroid\n    n_clusters - the number of cluster\n    \n    output:\n    labels - return an array of predictions on the cluster label of given features\n    centers - centroid values of each cluster\n    \"\"\"\n    model = KMeans(init=init, n_clusters = n_clusters, random_state = 42)\n\n    model.fit(df)\n\n    labels = model.predict(df)\n    \n    centers = np.array(model.cluster_centers_)\n    \n    return labels, centers\n\ndef percentage_row(df):\n    \"\"\"\n    input:\n    df - target dataframe\n    \n    output - a new dataframe in which each cell represents the row \n    percengatge value of the corresponding one in the target dataframe\n    \n    \"\"\"\n    new_df = df.div(df.sum(axis=1), axis=0)\n    new_df_share = round(new_df.apply(lambda x: x*100), 1).reset_index()\n    return new_df_share\n\ndef cluster_aggr(df, cols):\n    \"\"\"\n    input\n    df: target dataset\n    \n    cols: columns of the question we are interested to see the segmentation\n    \n    output:\n    a new dataframe that contains the number of participants for each question option\n    \"\"\"\n    aggr = df.groupby(['cluster']).sum()\n    aggr_col = aggr.iloc[:, cols]\n    aggr_col = aggr_col.loc[:, (aggr_col != 0).any(axis=0)]\n    aggr_col.loc[\"Total\"] = aggr_col.sum()\n\n    \n    return aggr_col\n\ndef plot_bar_perc(df, cols):\n    \"\"\"\n    input:\n    df - target dataframe\n    cols - columns we want to present as bars in the outcome chart\n    \n    output:\n    a bar chart where each bar represents the share of each value in the column aggregated by cluster\n    \"\"\"\n    fig = make_subplots(rows=1, cols=3, \n                    start_cell=\"bottom-left\", \n                        shared_yaxes=True,\n                    subplot_titles=(cluster_title))\n\n    clusters = df.index.tolist()\n\n    options = df.columns[1:]\n\n    colors = single_blue * len(options)\n\n    for c in range(len(clusters)):\n        data = df.loc[df['cluster'] == clusters[c]] \n        titles = []\n        for o in range(len(options)):\n            titles.append(options[o])\n            fig.add_trace(go.Bar(x=[options[o].split(\"- Selected Choice -\")[-1].strip()], \n                             y=data[options[o]],\n                             marker_color = colors[o],\n                                name = \"\"),\n                              row=1, col=c+1)\n        \n    fig.update_layout(\n        title=titles[0].split(\"- Selected Choice -\")[0],\n        yaxis_title=\"% of participants\",\n        showlegend=False)\n    #fig.write_image(\"visualizations/\" + str(cols[1])[:8] + \".jpeg\")\n    fig.show()\n    \ndef cluster_question_plot(df, question):\n    \"\"\"\n    input:\n    df - target dataset\n    \n    question - the question we are interested to segmented by the cluster\n    \n    output:\n    a list which contains a table and a plot showing the share of each segment per cluster\n    \"\"\"\n    aggr_data = cluster_aggr(df, range(qs_num[question][0], qs_num[question][1]))\n    aggr_perc = percentage_row(aggr_data)\n    plot_data = aggr_perc.loc[aggr_perc['cluster'].isin([0,1,2])]\n    \n    plot_chart = plot_bar_perc(plot_data, plot_data.columns[1:])\n    \n    return aggr_perc, plot_chart\n\ndef plot_bar_rank(df, cols, num_col = 10):\n    \"\"\"\n    input\n    df: target dataframe\n    cols : Question you want to aggregate\n    num_col: number of options shown in the chart\n    \n    output:\n    return a bar chart where options with highest total shares are set at the left side\n    \"\"\"\n    data_aggr = cluster_aggr(df, range(qs_num[cols][0], qs_num[cols][1]))\n    data_aggr = percentage_row(data_aggr)\n    data_aggr_rank = rank_total(data_aggr)\n\n    top_data_aggr_rank_cols = ['cluster']\n    for col in data_aggr_rank.columns:\n        top_data_aggr_rank_cols.append(col)\n    \n    aggr_cols = data_aggr.columns\n\n    data_aggr = data_aggr.loc[data_aggr['cluster'].isin([0,1,2])]\n\n    plot_bar_perc(data_aggr[top_data_aggr_rank_cols[:num_col]], aggr_cols)\n\ndef std_cluster(df):\n    \"\"\"\n    input:\n    df - target dataframe\n    output:\n    std - standard deviation of each row per cluster\n    \"\"\"\n    std = df.iloc[:,1:].std(axis=1)\n    return std\n\ndef rank_total(df):\n    \"\"\"\n    input: target dataframe\n    \n    output: a new dataframe which columns are ranked by the value in the Total row, so higher values are set at the left side\n    \"\"\"\n    df = df.iloc[:,1:] # remove cluster column\n    df_ranked = df.sort_values(by = 3, axis=1 , ascending = False)\n    return df_ranked\n\ndef find_correlation_rank(df,col,ascending = False):\n    \"\"\"\n    input:\n    df - target dataframe\n    col - column in interest\n    \n    output:\n    a list of columns in which the highest positive correlated col ranks the first\n    \"\"\"\n    df_ranked = df[[col]].sort_values(by = col,ascending = ascending)\n    \n    return df_ranked\n\ndef remove_col_zero(df):\n    \"\"\"\n    input:\n    df - target dataframe\n    \n    output:\n    a new dataframe that has all columns of sum 0 removed from the df\n    \"\"\"\n    new_df = df.loc[(df.sum(axis=1) != 0), (df.sum(axis=0) != 0)]\n    \n    return new_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-12T14:43:46.483609Z","iopub.execute_input":"2022-10-12T14:43:46.484552Z","iopub.status.idle":"2022-10-12T14:43:49.259247Z","shell.execute_reply.started":"2022-10-12T14:43:46.484505Z","shell.execute_reply":"2022-10-12T14:43:49.257833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# color palatte for visualization\nshades_blue = ['#90EE90','#00FF7F','#00FFFF','#89CFF0','#1434A4','#0096FF',\n               '#6495ED','#1F51FF','#2F4F4F','#A7C7E7','#00008B']\n\nsingle_blue = ['#89CFF0']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-12T14:43:49.260861Z","iopub.execute_input":"2022-10-12T14:43:49.261171Z","iopub.status.idle":"2022-10-12T14:43:49.266535Z","shell.execute_reply.started":"2022-10-12T14:43:49.261144Z","shell.execute_reply":"2022-10-12T14:43:49.265404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load dataset\ndata = pd.read_csv(\"/kaggle/input/kaggle-survey-2022/kaggle_survey_2022_responses.csv\")\n\n# first five rows\n# data.head()\n\n# remove the column Time from Start to Finish (seconds)\ndata = data.iloc[: , 1:]\n\n# size of the dataset\n# data.shape # 23,998 rows, 295 columns","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-12T14:43:49.26782Z","iopub.execute_input":"2022-10-12T14:43:49.268299Z","iopub.status.idle":"2022-10-12T14:43:50.765049Z","shell.execute_reply.started":"2022-10-12T14:43:49.268269Z","shell.execute_reply":"2022-10-12T14:43:50.763884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The dataset has \" + str(data.shape[0]) + \" rows.\")\n\nprint(\"The dataset has \" + str(data.shape[1]) + \" columns.\")","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:43:50.766918Z","iopub.execute_input":"2022-10-12T14:43:50.768266Z","iopub.status.idle":"2022-10-12T14:43:50.776183Z","shell.execute_reply.started":"2022-10-12T14:43:50.76821Z","shell.execute_reply":"2022-10-12T14:43:50.775038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning\n\nThe dataset is from Kaggle Machine Learning & Data Science Survey. According to the competition host, it has collected 23,998 valid answers from Kaggle users. Kaggle is a free online data science community where participants could attend data science competitions. Its annual survey is representative to understand professionists in the data science world.","metadata":{}},{"cell_type":"code","source":"# make question as columne names\nrenamed_data = rename_columns(data)\n\n# group questions into two categories\n# single_questions if it is a single answer question\n# multiple_questions if it is a multiple answer question\nsingle_questions = split_cols(renamed_data)[0]\nmultiple_questions = split_cols(renamed_data)[1]\n\n# replace values for simplification\nrenamed_data = renamed_data.replace(\"Prefer to self-describe\", \"self describe\")\nrenamed_data = renamed_data.replace(\"United Kingdom of Great Britain and Northern Ireland\", \"UK\")\nrenamed_data = renamed_data.replace(\"United States of America\", \"the U.S.\")\n\n# add question into the value of single option quetions\n# This is aligning the format of single option questions with the multiple option ones.\nfor col in renamed_data.columns:\n    if col in single_questions:\n        renamed_data[col] = col + \"- Selected Choice -\" + renamed_data[col]\n    else:\n        pass\n\n# pivot the dataset to one option one colum\npivoted_data = pivot_df(renamed_data)\n\n# turn answer as binary data where chosen is 1 and not chosen is 0 \nreplace_nan_data = replace_nan(pivoted_data)\n\n# remove columns that have sum as 0\nbinary_data = remove_col_zero(replace_nan_data)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:43:50.779724Z","iopub.execute_input":"2022-10-12T14:43:50.781154Z","iopub.status.idle":"2022-10-12T14:43:59.778655Z","shell.execute_reply.started":"2022-10-12T14:43:50.7811Z","shell.execute_reply":"2022-10-12T14:43:59.777271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Question and its number of columns\nqs_num = {\n    \"Age\" : [0,11], #\n    \"Gender\" : [11,16], #\n    \"Country\" : [16,74], #\n    \"IsStudent\" : [74,76], #  \n    \"LearnPlt\" : [76,88], #\n    \"FirstLearn\" : [88,95], #\n    \"HighEdu\" : [95,102], #\n    \"IsPublish\" : [102,104], #  \n    \"ResML\" : [104,107], #  \n    \"CodeExp\" : [107,114],#\n    \"ProgLangReg\" : [114,129], #\n    \"IDE\" : [129,143], #\n    \"HostNotebook\" : [143, 159], #\n    \"VisualLib\" : [159,174], #\n    \"MLmethd\" : [174,183], #\n    \"MLframe\" : [183,198], #\n    \"MLalgorithm\" : [198,212], #\n    \"CompVis\" : [212,220], #\n    \"NLP\" : [220,226], #\n    \"PreTrainWgt\" : [226,236], #\n    \"MLhub\" : [236,245], #\n    \"Employment\" : [245,260], #\n    \"Industry\" : [260,275], #\n    \"SizeEmployer\" : [275,280], #\n    \"SizeDS\" : [280,287], #?\n    \"DSBusiness\" : [287,293], #\n    \"WorkAct\" : [293,301], #\n    \"Compensation\" : [301,327], #\n    \"InvestDS\" : [327, 333], #\n    \"CldCompPltReg\" : [333,345], #\n    \"CldCompPltBstExp\" : [345,358], #\n    \"CldCompProdReg\" : [358,363], #\n    \"DataStoreProdReg\" : [363,371], #\n    \"BigDataProdReg\" : [371,387], #\n    \"IntegenceReg\" : [387,402], #\n    \"ManageMLProdReg\" : [402,415], #\n    \"AutoMLReg\" : [415,423], #\n    \"MLModelProd\" : [423,435], #\n    \"MLMonitor\" : [435,450], #\n    \"AIEthic\" : [450,459], #\n    \"HardwareReg\" : [459,468], #\n    \"TPUtimes\" : [468,473], #\n    \"FavMedia\" : [473,485] #\n}","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:43:59.780258Z","iopub.execute_input":"2022-10-12T14:43:59.780648Z","iopub.status.idle":"2022-10-12T14:43:59.792084Z","shell.execute_reply.started":"2022-10-12T14:43:59.780614Z","shell.execute_reply":"2022-10-12T14:43:59.790797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method\n\nThis analysis adopts K-means to find participant clusters based on the pattern how they respond to the survey. It aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster [link](https://en.wikipedia.org/wiki/K-means_clustering).","metadata":{}},{"cell_type":"code","source":"# explore how many number cluster can give a small enough inertia and also be as small number as possible\nkmeans_cluster_opt(binary_data)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:43:59.793702Z","iopub.execute_input":"2022-10-12T14:43:59.794508Z","iopub.status.idle":"2022-10-12T14:44:42.198205Z","shell.execute_reply.started":"2022-10-12T14:43:59.794471Z","shell.execute_reply":"2022-10-12T14:44:42.196902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a new column cluster segmenting participants\n# As the chart indicates above, we choose to make 3 clusters\nbinary_data['cluster'] = kmeans_predict(binary_data , n_clusters = 3)[0]","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-12T14:44:42.200402Z","iopub.execute_input":"2022-10-12T14:44:42.200785Z","iopub.status.idle":"2022-10-12T14:44:45.708819Z","shell.execute_reply.started":"2022-10-12T14:44:42.200746Z","shell.execute_reply":"2022-10-12T14:44:45.707854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for visualization use later\n# cluster 0 are explorers\n# cluster 1 are climbers\n# cluster 2 are experts\ncluster_title = [\"explorers\", \"climbers\", \"experts\"]","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:45.710261Z","iopub.execute_input":"2022-10-12T14:44:45.714211Z","iopub.status.idle":"2022-10-12T14:44:45.720025Z","shell.execute_reply.started":"2022-10-12T14:44:45.714169Z","shell.execute_reply":"2022-10-12T14:44:45.719007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis\nFirst of all, let's take a look at how many participants per each cluster.","metadata":{}},{"cell_type":"code","source":"binary_data.groupby(['cluster']).size()","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:45.721539Z","iopub.execute_input":"2022-10-12T14:44:45.722223Z","iopub.status.idle":"2022-10-12T14:44:45.739464Z","shell.execute_reply.started":"2022-10-12T14:44:45.722178Z","shell.execute_reply":"2022-10-12T14:44:45.737829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 10,633 explorers, 8,723 climbers and 4,641 experts.","metadata":{}},{"cell_type":"markdown","source":"### Demographics","metadata":{}},{"cell_type":"markdown","source":"Climbers and explorers have higher shares at student age zone, which is between 18-24 year old than expert particitpants. For experts, the highest share is 25-29, ~18% of total, which might imply they have more working experience or higher educational levels.","metadata":{}},{"cell_type":"code","source":"age_aggr = cluster_aggr(binary_data, range(qs_num[\"Age\"][0], qs_num[\"Age\"][1]))\nage_aggr = percentage_row(age_aggr)\n\ncols = age_aggr.columns\n\ncols = ['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-69','70+']\n\nage_aggr = age_aggr.loc[age_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(age_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:45.742113Z","iopub.execute_input":"2022-10-12T14:44:45.742969Z","iopub.status.idle":"2022-10-12T14:44:46.331406Z","shell.execute_reply.started":"2022-10-12T14:44:45.742929Z","shell.execute_reply":"2022-10-12T14:44:46.330527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regardless of the cluster, men practitioners are dominant in the industry. The percentage gap betweeen man and other categories go even wider in the experts group.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"Gender\",10)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:46.332894Z","iopub.execute_input":"2022-10-12T14:44:46.3335Z","iopub.status.idle":"2022-10-12T14:44:46.74031Z","shell.execute_reply.started":"2022-10-12T14:44:46.333464Z","shell.execute_reply":"2022-10-12T14:44:46.739421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"India has the highest share of participants in each cluster, followed by the Unitde States. The gap between India and the U.S. comes to the closest in the experts cluster.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"Country\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:46.741737Z","iopub.execute_input":"2022-10-12T14:44:46.742313Z","iopub.status.idle":"2022-10-12T14:44:47.035567Z","shell.execute_reply.started":"2022-10-12T14:44:46.742278Z","shell.execute_reply":"2022-10-12T14:44:47.034072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All experts are no students and most of the climbers are students. For explorers, students have a slight higher share than non-student.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"IsStudent\",10)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:47.040328Z","iopub.execute_input":"2022-10-12T14:44:47.040739Z","iopub.status.idle":"2022-10-12T14:44:47.267944Z","shell.execute_reply.started":"2022-10-12T14:44:47.040704Z","shell.execute_reply":"2022-10-12T14:44:47.266942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experts have the highest share in Doctoral and Master degree than any other two.","metadata":{}},{"cell_type":"code","source":"# order the columns in a reasonable manner\n\nedu_aggr = cluster_aggr(binary_data, range(qs_num[\"HighEdu\"][0], qs_num[\"HighEdu\"][1]))\nedu_aggr = percentage_row(edu_aggr)\n\ncols = edu_aggr.columns.tolist()\n\ncols_order = [\n 'cluster',\n 'No formal education past high school',\n 'Some college/university study without earning a bachelor’s degree',\n 'Bachelor’s degree',\n 'Master’s degree',\n 'Professional doctorate',\n 'Doctoral degree',\n 'I prefer not to answer'\n ]\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n\nedu_aggr = edu_aggr[cols_final]\n\nedu_aggr = edu_aggr.loc[edu_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(edu_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:47.269533Z","iopub.execute_input":"2022-10-12T14:44:47.270232Z","iopub.status.idle":"2022-10-12T14:44:47.525668Z","shell.execute_reply.started":"2022-10-12T14:44:47.270188Z","shell.execute_reply":"2022-10-12T14:44:47.524437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With no doubt, experts have the highest share that some of them have research paper published.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"IsPublish\",10)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:47.527409Z","iopub.execute_input":"2022-10-12T14:44:47.527802Z","iopub.status.idle":"2022-10-12T14:44:47.755428Z","shell.execute_reply.started":"2022-10-12T14:44:47.527726Z","shell.execute_reply":"2022-10-12T14:44:47.754302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Explorers have the largest share reporting their published research paper has nothing to do with machine learning.\nThe choice proportion of climbers and experts are relatively close in this question.","metadata":{}},{"cell_type":"code","source":"res_paper = cluster_aggr(binary_data, range(qs_num[\"ResML\"][0], qs_num[\"ResML\"][1]))\nres_paper = percentage_row(res_paper)\n\ncols = res_paper.columns.tolist()\n\ncols_order = [\n 'cluster',\n 'No',\n 'Yes, the research made use of machine learning as a tool (applied research)',\n 'Yes, the research made advances related to some novel machine learning method (theoretical research)'\n ]\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n\nres_paper = res_paper[cols_final]\n\nres_paper = res_paper.loc[res_paper['cluster'].isin([0,1,2])]\n\nplot_bar_perc(res_paper, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:47.756766Z","iopub.execute_input":"2022-10-12T14:44:47.757087Z","iopub.status.idle":"2022-10-12T14:44:47.98915Z","shell.execute_reply.started":"2022-10-12T14:44:47.757057Z","shell.execute_reply":"2022-10-12T14:44:47.987923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's interesting to see university courses are never in top 3 for any cluster as their first helpful learning source into data science. Experts and explorers have online courses as the top 1 while climbers see Kaggle as the top priority.","metadata":{}},{"cell_type":"markdown","source":"### Skillset and knowledge","metadata":{}},{"cell_type":"markdown","source":"Climbers and experts have coding experience more or less, while ~20% of explorers have not coded before. Experts have more shares of veterans than climbers.","metadata":{}},{"cell_type":"code","source":"codeExp_aggr = cluster_aggr(binary_data, range(qs_num[\"CodeExp\"][0], qs_num[\"CodeExp\"][1]))\ncodeExp_aggr = percentage_row(codeExp_aggr)\n\ncols = codeExp_aggr.columns.tolist()\n\ncols_order = ['cluster',\n 'I have never written code',\n '< 1 years',\n '1-3 years',\n '3-5 years',\n '5-10 years',\n '10-20 years',\n '20+ years']\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n            \ncodeExp_aggr = codeExp_aggr[cols_final]\n\ncodeExp_aggr = codeExp_aggr.loc[codeExp_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(codeExp_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:47.990618Z","iopub.execute_input":"2022-10-12T14:44:47.99096Z","iopub.status.idle":"2022-10-12T14:44:48.249322Z","shell.execute_reply.started":"2022-10-12T14:44:47.990929Z","shell.execute_reply":"2022-10-12T14:44:48.248163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Programming languages that experts have reported with higher share than any other cluster are SQL, R, Javascript and Bash. For SQL and R, the reasons could be those are the most frequently used tools for data manipulation and statistics, and therefore Experts have more experience with. For JS, the share is very close among the threes and I doubt there is significant difference. For Bash, it's one of the most common languages if you want to develop something into production.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"ProgLangReg\",10)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:48.250677Z","iopub.execute_input":"2022-10-12T14:44:48.251076Z","iopub.status.idle":"2022-10-12T14:44:48.510024Z","shell.execute_reply.started":"2022-10-12T14:44:48.251047Z","shell.execute_reply":"2022-10-12T14:44:48.508593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Explorers have the highest share that reports using none visualization library and it's probably due to their highest percentage of participants have no coding experience. Compared with climbers, experts have more evenly distributed usage of visualization libraries.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"VisualLib\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:48.511383Z","iopub.execute_input":"2022-10-12T14:44:48.511719Z","iopub.status.idle":"2022-10-12T14:44:48.790435Z","shell.execute_reply.started":"2022-10-12T14:44:48.511689Z","shell.execute_reply":"2022-10-12T14:44:48.789317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"MLmethd_aggr = cluster_aggr(binary_data, range(qs_num[\"MLmethd\"][0], qs_num[\"MLmethd\"][1]))\nMLmethd_aggr = percentage_row(MLmethd_aggr)\n\ncols = MLmethd_aggr.columns.tolist()\ncols_order = ['cluster',\n 'I do not use machine learning methods',\n 'Under 1 year',\n '1-2 years',\n '2-3 years',\n '3-4 years',\n '4-5 years',\n '5-10 years',\n '10-20 years',\n '20 or more years']\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n\nMLmethd_aggr = MLmethd_aggr[cols_final]\n\nMLmethd_aggr = MLmethd_aggr.loc[MLmethd_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(MLmethd_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:45:23.434619Z","iopub.execute_input":"2022-10-12T14:45:23.435042Z","iopub.status.idle":"2022-10-12T14:45:23.707967Z","shell.execute_reply.started":"2022-10-12T14:45:23.43501Z","shell.execute_reply":"2022-10-12T14:45:23.70648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experts tend to use less popular Machine Learning frameworks, compared with climbers.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"MLframe\",18)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:48:39.166652Z","iopub.execute_input":"2022-10-12T14:48:39.167179Z","iopub.status.idle":"2022-10-12T14:48:39.556179Z","shell.execute_reply.started":"2022-10-12T14:48:39.167141Z","shell.execute_reply":"2022-10-12T14:48:39.554662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"MLalgorithm\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:48:29.946449Z","iopub.execute_input":"2022-10-12T14:48:29.946875Z","iopub.status.idle":"2022-10-12T14:48:30.229921Z","shell.execute_reply.started":"2022-10-12T14:48:29.94684Z","shell.execute_reply":"2022-10-12T14:48:30.228735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"CompVis\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:49:12.046804Z","iopub.execute_input":"2022-10-12T14:49:12.047282Z","iopub.status.idle":"2022-10-12T14:49:12.312809Z","shell.execute_reply.started":"2022-10-12T14:49:12.047248Z","shell.execute_reply":"2022-10-12T14:49:12.31148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"NLP\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:54:09.10047Z","iopub.execute_input":"2022-10-12T14:54:09.100878Z","iopub.status.idle":"2022-10-12T14:54:09.351916Z","shell.execute_reply.started":"2022-10-12T14:54:09.100846Z","shell.execute_reply":"2022-10-12T14:54:09.35082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Profession","metadata":{}},{"cell_type":"markdown","source":"Climbers has the highest share participants are not employed currently, because they have a highest student share. For experts, the largest share goes to data scientists and they also have higher shares in other two close options, machine learning engineers and research scientist. Experts also have higher share in teacher/professor, and that's probably why they have highest share owning publishing experience.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"Employment\",10)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:48.792198Z","iopub.execute_input":"2022-10-12T14:44:48.792655Z","iopub.status.idle":"2022-10-12T14:44:49.05158Z","shell.execute_reply.started":"2022-10-12T14:44:48.79261Z","shell.execute_reply":"2022-10-12T14:44:49.050465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# plot_bar_rank(binary_data, \"WorkAct\",10)\n# neeed to clean the x axis","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:49.053447Z","iopub.execute_input":"2022-10-12T14:44:49.053782Z","iopub.status.idle":"2022-10-12T14:44:49.057511Z","shell.execute_reply.started":"2022-10-12T14:44:49.053753Z","shell.execute_reply":"2022-10-12T14:44:49.056538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experts have the highest share go to Computers and Tech, which is also the top one for the other two clusters.\n\nOther than that, Accounting and Finance and Medical aree also two industries where experts are more likely to work in.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"Industry\",25)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:49.059211Z","iopub.execute_input":"2022-10-12T14:44:49.060087Z","iopub.status.idle":"2022-10-12T14:44:49.348986Z","shell.execute_reply.started":"2022-10-12T14:44:49.060028Z","shell.execute_reply":"2022-10-12T14:44:49.347702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"size_emp_aggr = cluster_aggr(binary_data, range(qs_num[\"SizeEmployer\"][0], qs_num[\"SizeEmployer\"][1]))\nsize_emp_aggr = percentage_row(size_emp_aggr)\n\ncols = size_emp_aggr.columns.tolist()\n\n\ncols_order = ['cluster',\n '0-49 employees',\n '50-249 employees',\n '250-999 employees',\n '1000-9,999 employees',\n '10,000 or more employees']\n\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n\nsize_emp_aggr = size_emp_aggr[cols_final]\n\nsize_emp_aggr = size_emp_aggr.loc[size_emp_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(size_emp_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:49.35066Z","iopub.execute_input":"2022-10-12T14:44:49.351085Z","iopub.status.idle":"2022-10-12T14:44:49.597011Z","shell.execute_reply.started":"2022-10-12T14:44:49.35105Z","shell.execute_reply":"2022-10-12T14:44:49.595891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experts are more likely to work in employers that havee applied machine learning into production.","metadata":{}},{"cell_type":"code","source":"ds_business_aggr = cluster_aggr(binary_data, range(qs_num[\"DSBusiness\"][0], qs_num[\"DSBusiness\"][1]))\nds_business_aggr = percentage_row(ds_business_aggr)\n\ncols = ds_business_aggr.columns.tolist()\n\ncols_order = ['cluster',\n 'No (we do not use ML methods)',\n 'We use ML methods for generating insights (but do not put working models into production)',\n 'We are exploring ML methods (and may one day put a model into production)',\n 'We recently started using ML methods (i.e., models in production for less than 2 years)',\n 'We have well established ML methods (i.e., models in production for more than 2 years)',\n 'I do not know']\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n\nds_business_aggr = ds_business_aggr[cols_final]\n\nds_business_aggr = ds_business_aggr.loc[ds_business_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(ds_business_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:49.598525Z","iopub.execute_input":"2022-10-12T14:44:49.598884Z","iopub.status.idle":"2022-10-12T14:44:49.845435Z","shell.execute_reply.started":"2022-10-12T14:44:49.598853Z","shell.execute_reply":"2022-10-12T14:44:49.84432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experts earn more.","metadata":{}},{"cell_type":"code","source":"compensation_aggr = cluster_aggr(binary_data, range(qs_num[\"Compensation\"][0], qs_num[\"Compensation\"][1]))\ncompensation_aggr = percentage_row(compensation_aggr)\n\ncols = compensation_aggr.columns.tolist()\ncols_order = ['cluster',\n '$0-999',\n '1,000-1,999',\n '2,000-2,999',\n '3,000-3,999',\n '4,000-4,999',\n '5,000-7,499',\n '7,500-9,999',\n '10,000-14,999',\n '15,000-19,999',\n '20,000-24,999',\n '25,000-29,999',\n '30,000-39,999',\n '40,000-49,999',\n '50,000-59,999',\n '60,000-69,999',\n '70,000-79,999',\n '80,000-89,999',\n '90,000-99,999',\n '100,000-124,999',\n '125,000-149,999',\n '150,000-199,999',\n '200,000-249,999',\n '250,000-299,999',\n '300,000-499,999',\n '$500,000-999,999',\n '>$1,000,000']\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n\ncompensation_aggr = compensation_aggr[cols_final]\n\ncompensation_aggr = compensation_aggr.loc[compensation_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(compensation_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:49.846957Z","iopub.execute_input":"2022-10-12T14:44:49.847301Z","iopub.status.idle":"2022-10-12T14:44:50.186161Z","shell.execute_reply.started":"2022-10-12T14:44:49.84727Z","shell.execute_reply":"2022-10-12T14:44:50.185015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tools","metadata":{}},{"cell_type":"markdown","source":"Experts have high share reporting that they used Jupyterlab (what is the difference between this one Jupyter Notebook?), and Vim / Emacs. It's related with their regular programming language.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"IDE\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:50.187559Z","iopub.execute_input":"2022-10-12T14:44:50.187895Z","iopub.status.idle":"2022-10-12T14:44:50.46802Z","shell.execute_reply.started":"2022-10-12T14:44:50.187865Z","shell.execute_reply":"2022-10-12T14:44:50.466563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"HostNotebook\",20)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:50.469784Z","iopub.execute_input":"2022-10-12T14:44:50.470165Z","iopub.status.idle":"2022-10-12T14:44:50.757051Z","shell.execute_reply.started":"2022-10-12T14:44:50.470133Z","shell.execute_reply":"2022-10-12T14:44:50.755562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"invest_ds_aggr = cluster_aggr(binary_data, range(qs_num[\"InvestDS\"][0], qs_num[\"InvestDS\"][1]))\ninvest_ds_aggr = percentage_row(invest_ds_aggr)\n\ncols = invest_ds_aggr.columns.tolist()\ncols_order = ['cluster',\n '$0 ($USD)',\n '$1-$99',\n '$100-$999',\n '$1000-$9,999',\n '$10,000-$99,999',\n '$100,000 or more ($USD)']\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n            \ninvest_ds_aggr = invest_ds_aggr[cols_final]\n\ninvest_ds_aggr = invest_ds_aggr.loc[invest_ds_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(invest_ds_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:50.758754Z","iopub.execute_input":"2022-10-12T14:44:50.759905Z","iopub.status.idle":"2022-10-12T14:44:51.006286Z","shell.execute_reply.started":"2022-10-12T14:44:50.759864Z","shell.execute_reply":"2022-10-12T14:44:51.005425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"HardwareReg\",10)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:51.00741Z","iopub.execute_input":"2022-10-12T14:44:51.007949Z","iopub.status.idle":"2022-10-12T14:44:51.264512Z","shell.execute_reply.started":"2022-10-12T14:44:51.007914Z","shell.execute_reply":"2022-10-12T14:44:51.263319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TPUtimes_aggr = cluster_aggr(binary_data, range(qs_num[\"TPUtimes\"][0], qs_num[\"TPUtimes\"][1]))\nTPUtimes_aggr = percentage_row(TPUtimes_aggr)\n\ncols = TPUtimes_aggr.columns.tolist()\ncols_order = ['cluster',\n        'Never',\n        'Once',\n        '2-5 times',\n        '6-25 times',\n        'More than 25 times']\n\ncols_final = []\nfor col_1 in cols_order:\n    col_1_index = cols_order.index(col_1)\n    for col_2 in cols:\n        if col_1 in col_2:\n            cols_final.insert(col_1_index, col_2)\n\nTPUtimes_aggr = TPUtimes_aggr[cols_final]\n\nTPUtimes_aggr = TPUtimes_aggr.loc[TPUtimes_aggr['cluster'].isin([0,1,2])]\n\nplot_bar_perc(TPUtimes_aggr, cols)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:51.26619Z","iopub.execute_input":"2022-10-12T14:44:51.266647Z","iopub.status.idle":"2022-10-12T14:44:51.508698Z","shell.execute_reply.started":"2022-10-12T14:44:51.266605Z","shell.execute_reply":"2022-10-12T14:44:51.50758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"PreTrainWgt\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:55:52.682539Z","iopub.execute_input":"2022-10-12T14:55:52.683061Z","iopub.status.idle":"2022-10-12T14:55:53.00369Z","shell.execute_reply.started":"2022-10-12T14:55:52.683025Z","shell.execute_reply":"2022-10-12T14:55:53.002538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"MLhub\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:56:00.19663Z","iopub.execute_input":"2022-10-12T14:56:00.197481Z","iopub.status.idle":"2022-10-12T14:56:00.553051Z","shell.execute_reply.started":"2022-10-12T14:56:00.197419Z","shell.execute_reply":"2022-10-12T14:56:00.552099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"CldCompPltReg\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:51.51025Z","iopub.execute_input":"2022-10-12T14:44:51.511357Z","iopub.status.idle":"2022-10-12T14:44:51.780591Z","shell.execute_reply.started":"2022-10-12T14:44:51.51129Z","shell.execute_reply":"2022-10-12T14:44:51.779398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"CldCompPltBstExp\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:02:09.299882Z","iopub.execute_input":"2022-10-12T15:02:09.300485Z","iopub.status.idle":"2022-10-12T15:02:09.689429Z","shell.execute_reply.started":"2022-10-12T15:02:09.300438Z","shell.execute_reply":"2022-10-12T15:02:09.688234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"CldCompProdReg\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:03:33.962333Z","iopub.execute_input":"2022-10-12T15:03:33.963756Z","iopub.status.idle":"2022-10-12T15:03:34.312451Z","shell.execute_reply.started":"2022-10-12T15:03:33.963694Z","shell.execute_reply":"2022-10-12T15:03:34.31107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"DataStoreProdReg\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:04:42.669433Z","iopub.execute_input":"2022-10-12T15:04:42.669954Z","iopub.status.idle":"2022-10-12T15:04:43.028135Z","shell.execute_reply.started":"2022-10-12T15:04:42.669917Z","shell.execute_reply":"2022-10-12T15:04:43.026781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"BigDataProdReg\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:04:43.030252Z","iopub.execute_input":"2022-10-12T15:04:43.031731Z","iopub.status.idle":"2022-10-12T15:04:43.39822Z","shell.execute_reply.started":"2022-10-12T15:04:43.031675Z","shell.execute_reply":"2022-10-12T15:04:43.396833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"IntegenceReg\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:04:43.400694Z","iopub.execute_input":"2022-10-12T15:04:43.401114Z","iopub.status.idle":"2022-10-12T15:04:43.697923Z","shell.execute_reply.started":"2022-10-12T15:04:43.401075Z","shell.execute_reply":"2022-10-12T15:04:43.696412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"ManageMLProdReg\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:04:43.69965Z","iopub.execute_input":"2022-10-12T15:04:43.700131Z","iopub.status.idle":"2022-10-12T15:04:44.009736Z","shell.execute_reply.started":"2022-10-12T15:04:43.700083Z","shell.execute_reply":"2022-10-12T15:04:44.008448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"AutoMLReg\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:04:44.259788Z","iopub.execute_input":"2022-10-12T15:04:44.260237Z","iopub.status.idle":"2022-10-12T15:04:44.518174Z","shell.execute_reply.started":"2022-10-12T15:04:44.260197Z","shell.execute_reply":"2022-10-12T15:04:44.516325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"MLModelProd\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:07:56.889507Z","iopub.execute_input":"2022-10-12T15:07:56.88995Z","iopub.status.idle":"2022-10-12T15:07:57.177474Z","shell.execute_reply.started":"2022-10-12T15:07:56.889917Z","shell.execute_reply":"2022-10-12T15:07:57.176434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"MLMonitor\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:07:57.179212Z","iopub.execute_input":"2022-10-12T15:07:57.180019Z","iopub.status.idle":"2022-10-12T15:07:57.461116Z","shell.execute_reply.started":"2022-10-12T15:07:57.179974Z","shell.execute_reply":"2022-10-12T15:07:57.459872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"AIEthic\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T15:07:57.463254Z","iopub.execute_input":"2022-10-12T15:07:57.464538Z","iopub.status.idle":"2022-10-12T15:07:57.727658Z","shell.execute_reply.started":"2022-10-12T15:07:57.464489Z","shell.execute_reply":"2022-10-12T15:07:57.726391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning and development","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"FirstLearn\",10)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:52.035366Z","iopub.execute_input":"2022-10-12T14:44:52.035792Z","iopub.status.idle":"2022-10-12T14:44:52.286664Z","shell.execute_reply.started":"2022-10-12T14:44:52.035758Z","shell.execute_reply":"2022-10-12T14:44:52.285564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Coursera is the most popular among three clusters. For explorers, they have the highest share having None to learn, ~10%, which is almost 5 times as the equivalent of climbers and experts. Climbers prefer university courses as the second option since they have the highest share being students, on the other hand, experts also learn online courses on Udemy.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"LearnPlt\",10)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:52.288723Z","iopub.execute_input":"2022-10-12T14:44:52.289175Z","iopub.status.idle":"2022-10-12T14:44:52.548062Z","shell.execute_reply.started":"2022-10-12T14:44:52.289122Z","shell.execute_reply":"2022-10-12T14:44:52.546907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Explorers prefer watching Youtube videos to teach themselves, while climbers and experts love playing at Kaggle. Compared with climbers, experts are more likely to read Blogs or Published Journals.","metadata":{}},{"cell_type":"code","source":"plot_bar_rank(binary_data, \"FavMedia\",15)","metadata":{"execution":{"iopub.status.busy":"2022-10-12T14:44:52.549756Z","iopub.execute_input":"2022-10-12T14:44:52.550945Z","iopub.status.idle":"2022-10-12T14:44:52.820015Z","shell.execute_reply.started":"2022-10-12T14:44:52.550898Z","shell.execute_reply":"2022-10-12T14:44:52.818759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will try to analyze the segments based on the result in the upcoming days. Stayed tuned!\n\nGood luck, everyone!","metadata":{}}]}